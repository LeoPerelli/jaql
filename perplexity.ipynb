{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils import quantise_model,get_model_memory_size, compute_quantisation_mse, Perplexity\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# model = model.half()\n",
    "\n",
    "text = [t for t in load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")['text'] if len(t) > 30]\n",
    "# get_model_memory_size(model)\n",
    "# model, parameter_mapping = quantise_model(model, chunk_size=1024)\n",
    "# get_model_memory_size(model, parameter_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "         [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "         [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "         ...,\n",
       "         [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "         [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "         [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones((1,10), dtype=torch.long)\n",
    "model = model.transformer.wte\n",
    "model.forward(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000512"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.097152"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_reserved()/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.named_parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = Perplexity()\n",
    "p = perplexity._compute(text, model, tokenizer)\n",
    "print(p['mean_perplexity'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put parameter_mapping on gpu\n",
    "for parameter in parameter_mapping.keys():\n",
    "    parameter_mapping[parameter]['scales'] = parameter_mapping[parameter]['scales'].to('cuda')\n",
    "    parameter_mapping[parameter]['locations'] = parameter_mapping[parameter]['locations'].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "set_seed(42)\n",
    "generator(\"Once upon a time, \", max_length=30, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0:1000\n",
    "fp32: 135.18940559005736 (40 seconds , 9533MiB memory, 100% gpu util)\n",
    "fp16: 135.193484375 (15 seconds, 4847MiB memory, 100% gpu util)\n",
    "int8_1024: 191.00541243076324 (took 9 minutes, used approx 9361 memory, very low gpu util (20% max)). putting the parameter_mappings on the gpu actually slows the inference down to 12 minutes\n",
    "\n",
    "0:100\n",
    "fp32: 188.55674095153807\n",
    "int8_256: 238.52101341247558\n",
    "int8_1024: 299.4311764335632\n",
    "\n",
    "moving model itself to cuda\n",
    "base: 642\n",
    "fp16: 368 (this is 1.15 times bigger than expected)\n",
    "int8_1024: 242 (this is 1.5 times bigger than expected) and this does not include 100MB of parameter_mapping, which should be like 2 MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W unwind.cpp:201] Warning: Unsupported unwinding pattern: Address not in range (function unwinderFor)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from utils import scalar_quantisation\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.cuda.memory._record_memory_history()\n",
    "chunk_size = 1000\n",
    "t = torch.rand((10000000))\n",
    "t = t.to('cuda')\n",
    "\n",
    "def q(t):\n",
    "    shape = t.shape\n",
    "    t_flat = t.flatten()\n",
    "    t_q = torch.zeros_like(t_flat).type(torch.int8)\n",
    "    n_chunks = math.ceil(len(t_flat) / chunk_size)\n",
    "    scales = torch.zeros(n_chunks)\n",
    "    locations = torch.zeros(n_chunks)\n",
    "\n",
    "    for chunk_id in range(n_chunks):\n",
    "\n",
    "        left = chunk_id * chunk_size\n",
    "        right = min(len(t_flat), (chunk_id + 1) * chunk_size)\n",
    "\n",
    "        t_q[left:right], scales[chunk_id], locations[chunk_id] = scalar_quantisation(t_flat[left:right])\n",
    "\n",
    "    t_q = t_q.reshape(shape)\n",
    "    return t_flat\n",
    "\n",
    "def q_inplace(t):\n",
    "    shape = t.shape\n",
    "    t_flat = t.flatten()\n",
    "    n_chunks = math.ceil(len(t_flat) / chunk_size)\n",
    "    scales = torch.zeros(n_chunks)\n",
    "    locations = torch.zeros(n_chunks)\n",
    "\n",
    "    for chunk_id in range(n_chunks):\n",
    "\n",
    "        left = chunk_id * chunk_size\n",
    "        right = min(len(t_flat), (chunk_id + 1) * chunk_size)\n",
    "\n",
    "        t_flat[left:right], scales[chunk_id], locations[chunk_id] = scalar_quantisation(t_flat[left:right])\n",
    "\n",
    "    t_flat = t_flat.reshape(shape)\n",
    "    t_flat = t_flat.type(torch.int8)\n",
    "\n",
    "    return t_flat\n",
    "\n",
    "qq = q(t)\n",
    "\n",
    "\n",
    "torch.cuda.memory._dump_snapshot(\"q.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    d = {'a': torch.rand((100000000)).to('cuda'), 'b':torch.rand((100000000)).to('cuda')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    del d['a']\n",
    "    del d['b']\n",
    "    del d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8888, 0.2352, 0.0999,  ..., 0.0175, 0.4840, 0.5250], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9091, 0.5111, 0.9969,  ..., 0.5063, 0.5423, 0.8254], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
